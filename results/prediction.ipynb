{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import os \n",
    "from os.path import join\n",
    "import shutil\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "basedir = '/Users/RobertAdragna/Documents/School/Fourth_Year/ESC499-Thesis/codebases/causal_discovery'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import data_processing as dp \n",
    "import models \n",
    "from utils import proc_fteng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '0525_smallsample'\n",
    "invariance_algos = {'irm':{}}  #,'icp':{}}\n",
    "non_invariance_algos = {'linreg':{}}\n",
    "\n",
    "for als in [invariance_algos, non_invariance_algos]:\n",
    "    for al in als.keys(): \n",
    "        als[al]['expdir'] = join(join(join(os.getcwd(), res_dir), al), 'causal_discovery')\n",
    "        als[al]['processed_dir'] = join(join(join(os.getcwd(), res_dir), al), 'processed_results')\n",
    "        als[al]['params'] = pd.read_pickle(join(join(join(os.getcwd(), res_dir), al), '{}_paramfile.pkl'.format(al)))\n",
    "\n",
    "        if not os.path.exists(als[al]['processed_dir']):\n",
    "            raise Exception('Directory has not yet been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariance_algos['irm']['params'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_binarize(v):\n",
    "    '''Convert all values to 0 if <0.5, 1 otherwise'''\n",
    "    def thresh(x):\n",
    "        if (x >= 0.5): return 1 \n",
    "        else: return 0\n",
    "    print(v.shape)\n",
    "    return np.apply_along_axis(thresh, 1, v)\n",
    "    \n",
    "        \n",
    "def compute_loss(pred, ground, ltype='MSE'):\n",
    "    '''Compute loss between two prediction vectors'''\n",
    "\n",
    "    \n",
    "    if ltype == 'MSE':\n",
    "        return F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    if ltype == 'ACC':\n",
    "        pred = pred_binarize(pred) \n",
    "        return 1 - F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    \n",
    "def fairness_dp(pred, ground, d, patts):\n",
    "    '''Compute demographic aparity wrt data\n",
    "    :param pred: vector, binary entries (np[float])\n",
    "    :param ground: vector, binary entries (np[float])\n",
    "    :param d: dataset (pandas df)\n",
    "    :param patt: datts dict {cat:[all orig columns]}'''\n",
    "    \n",
    "    #Get the protected attribute columns \n",
    "    assert len(patts.keys()) == 1\n",
    "    protected = [patts[cat] for cat in patts.keys()][0]\n",
    "    \n",
    "    #Compute p(y_hat=1 | a)  Va  (demographic parity)\n",
    "    probs = {}\n",
    "    for aval in protected: \n",
    "        if '_DUMmY' in aval:\n",
    "            subpop = (d[[a for a in protected if '_DUMmY' not in a]] == 0).all(1)\n",
    "        else:\n",
    "            subpop = (d[aval] == 1)\n",
    "        probs[aval] = pred[subpop].sum() / len(pred[subpop])\n",
    "    return probs\n",
    "    \n",
    "    #Compute p(y_hat=1 | a)  Va  (equalized odds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_subset(df, subset):\n",
    "    '''Get a subset of df rows whose columns specified in subset equal their respective values\n",
    "    :param df: Dataframe (pandas)\n",
    "    :param subset: Series of col_name:value pairs (pandas series)\n",
    "    '''\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col, val in pd.Series.iteritems(subset):\n",
    "        new_df = new_df[new_df[col] == val]\n",
    "    return new_df\n",
    "\n",
    "def get_dset_fname(dset, b):\n",
    "    if dset == 'adult':\n",
    "        datafname = join(join(b, 'data'), 'adult.csv')\n",
    "    elif dset == 'german':\n",
    "        datafname = join(join(b, 'data'), 'germanCredit.csv')\n",
    "    else:\n",
    "        raise Exception('Dataset unimplemented')\n",
    "    \n",
    "    return datafname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_existing_results(allcols, ags):\n",
    "    ''' param allcols: A list of the features to be included\n",
    "        param ags: A list of paramdfs for each algorithm'''\n",
    "    add = pd.DataFrame()\n",
    "    for param_df in ags: \n",
    "        if add.empty:\n",
    "            add = param_df[allcols]\n",
    "        else:\n",
    "            add = add.append(param_df[allcols], ignore_index=True)\n",
    "    \n",
    "    uniq = np.logical_not(add.duplicated())\n",
    "    return add[uniq]\n",
    "    \n",
    "    \n",
    "def generate_results(fixed, compared): \n",
    "    '''\n",
    "    :param fixed: A list of tuples (pname, pval) that are fixed across exps\n",
    "    :param compared: A dictionary of pname:full range of possible values in experiment ''' \n",
    "    \n",
    "    fixed_results = pd.Series([np.nan]*len(fixed), index=[f[0] for f in fixed]) #  , index=fixed_cols) \n",
    "    for f in fixed:\n",
    "        fixed_results[f[0]] = f[1]\n",
    "    \n",
    "    #Set Up the Results Dataframe \n",
    "    compared_results = pd.DataFrame(itertools.product(*[compared[cat] for cat in compared]))\n",
    "    compared_results.columns = list(compared.keys())\n",
    "    \n",
    "    #Set up the results \n",
    "    results = fixed_results.to_frame().T\n",
    "    results['key'] = 0 \n",
    "    compared_results['key'] = 0\n",
    "    results = results.merge(compared_results, on='key', how='inner')\n",
    "    results.drop('key', axis='columns', inplace=True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(algos, resdf, orig_cols, loss_fn='ACC'):\n",
    "    reddata = -1\n",
    "\n",
    "    for al in algos.keys():  #Enumerate through algos     \n",
    "        ##Hack\n",
    "        if ('{}-error'.format(al) in list(resdf.columns)) and ('{}-fairness'.format(al) in list(resdf.columns)):\n",
    "            resdf.drop('{}-error'.format(al), axis='columns', inplace=True) \n",
    "            resdf.drop('{}-fairness'.format(al), axis='columns', inplace=True) \n",
    "        #######\n",
    "\n",
    "        #Set up columns in dataframe\n",
    "        res_entries = ['{}-error'.format(al), '{}-fairness'.format(al)]\n",
    "        for r in res_entries:\n",
    "            resdf[r] = np.nan\n",
    "\n",
    "\n",
    "        for resid, row in resdf.iterrows():\n",
    "            rel = df_subset(algos[al]['params'], row[orig_cols])  #Get row-associated entry in param dframe\n",
    "            assert rel.shape[0] <= 1 #Guarentee just one (Excluding multi-index mappings)\n",
    "            if rel.shape[0] == 0: \n",
    "                continue\n",
    "\n",
    "            data, y_all, d_atts = dp.adult_dataset_processing(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                          proc_fteng(row['Fteng']), \\\n",
    "                                                          reduce_dsize=reddata, \\\n",
    "                                                          bin=row['Bin'])\n",
    "            #Compute Predictions  \n",
    "            if al == 'icp':\n",
    "                model = models.InvariantCausalPrediction()\n",
    "                learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'coeffs'])]\n",
    "                print(learned_model[0])\n",
    "                predictions = model.predict(data, *learned_model)    \n",
    "                if predictions.empty:\n",
    "                    resdf.loc[resid, '{}-error'.format(al)] = 'NA'\n",
    "                    resdf.loc[resid, '{}-fairness'.format(al)] = 'NA'\n",
    "                    continue\n",
    "\n",
    "            elif al == 'irm':\n",
    "                model = models.InvariantRiskMinimization()\n",
    "                learned_model = [torch.load(rel.loc[rel.index[0], 'phi'])]\n",
    "                predictions = model.predict(data.values, *learned_model)\n",
    "\n",
    "            elif al == 'linreg':\n",
    "                model = models.Linear()\n",
    "                learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'linregressors'])]\n",
    "                predictions = model.predict(data, *learned_model)   \n",
    "\n",
    "\n",
    "            #Compute Metrics on Predictions \n",
    "            import pdb; pdb.set_trace()\n",
    "            error = compute_loss(predictions.values, np.expand_dims(y_all.values, axis=1), ltype=loss_fn)\n",
    "            fairness =  str(fairness_dp(pred_binarize(predictions.values), np.expand_dims(y_all.values, axis=1),\\\n",
    "                                    data, {'race':d_atts['race']}))\n",
    "\n",
    "\n",
    "\n",
    "            #Save computed values to resdf \n",
    "            resdf.loc[resid, '{}-error'.format(al)] = error\n",
    "            resdf.loc[resid, '{}-fairness'.format(al)] = fairness\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Invariance Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invar_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Eq_Estrat', -1]] \n",
    "\n",
    "invar_COMPARED =  {'Envs':['workclass', 'native-country'], \\\n",
    "                   'Seed':[147, 256, 304],\n",
    "                   'Fteng':['1', '12'], \\\n",
    "                   'Bin':[1]}\n",
    "\n",
    "invar_orig_cols = [a[0] for a in invar_FIXED] + list(invar_COMPARED.keys())\n",
    "invar_results = generate_all_existing_results(invar_orig_cols, \\\n",
    "                                             [invariance_algos[a]['params'] for a in list(invariance_algos.keys())])    \n",
    "#invar_results = generate_results(invar_FIXED, invar_COMPARED)\n",
    "\n",
    "invar_results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(invariance_algos, invar_results, invar_orig_cols, loss_fn='ACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "invar_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Non-Invariance Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Bin', 1]] \n",
    "\n",
    "var_COMPARED =  {'Fteng':['1', '12'], \\\n",
    "                 'Seed':[147, 256, 304]}\n",
    "\n",
    "noninvar_results = generate_results(var_FIXED, var_COMPARED)\n",
    "noninvar_orig_cols = noninvar_results.columns\n",
    "\n",
    "noninvar_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(non_invariance_algos, noninvar_results, noninvar_orig_cols, loss_fn='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "noninvar_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
