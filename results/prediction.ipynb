{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import os \n",
    "from os.path import join\n",
    "import shutil\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "basedir = '/Users/RobertAdragna/Documents/School/Fourth_Year/ESC499-Thesis/codebases/causal_discovery'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import data_processing as dp \n",
    "import environment_processing as eproc \n",
    "import models \n",
    "from utils import proc_fteng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '0602_validation'\n",
    "invariance_algos = {'icp':{}}   #'icp':{},\n",
    "non_invariance_algos = {'linreg':{}}\n",
    "\n",
    "for als in [invariance_algos, non_invariance_algos]:\n",
    "    for al in als.keys(): \n",
    "        als[al]['expdir'] = join(join(join(os.getcwd(), res_dir), al), 'causal_discovery')\n",
    "        als[al]['processed_dir'] = join(join(join(os.getcwd(), res_dir), al), 'processed_results')\n",
    "        als[al]['params'] = pd.read_pickle(join(join(join(os.getcwd(), res_dir), al), '{}_paramfile.pkl'.format(al)))\n",
    "\n",
    "        if not os.path.exists(als[al]['processed_dir']):\n",
    "            raise Exception('Directory has not yet been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariance_algos['icp']['params'] = invariance_algos['icp']['params'].drop('2', axis=0)\n",
    "invariance_algos['icp']['params'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_binarize(v):\n",
    "    '''Convert all values to 0 if <0.5, 1 otherwise'''\n",
    "    def thresh(x):\n",
    "        if (x >= 0.5): return 1 \n",
    "        else: return 0\n",
    "    print(v.shape)\n",
    "    return np.apply_along_axis(thresh, 1, v)\n",
    "    \n",
    "        \n",
    "def compute_loss(pred, ground, ltype='MSE'):\n",
    "    '''Compute loss between two prediction vectors'''\n",
    "\n",
    "    \n",
    "    if ltype == 'MSE':\n",
    "        return F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    if ltype == 'ACC':\n",
    "        pred = pred_binarize(pred) \n",
    "        return 1 - F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    \n",
    "def fairness_dp(pred, ground, d, patts, ftype='DP'):\n",
    "    '''Compute demographic aparity wrt data\n",
    "    :param pred: vector, binary entries (np[float])\n",
    "    :param ground: vector, binary entries (np[float])\n",
    "    :param d: dataset (pandas df)\n",
    "    :param patt: datts dict {cat:[all orig columns]}'''\n",
    "    \n",
    "    def avg_diff_scores(p):\n",
    "        ''' Given a dictionary of scores for different sensitive attributes p.keys, \\ \n",
    "            return the average difference between these values '''\n",
    "        na = len(p.keys())\n",
    "        n_combos = math.factorial(na)/ (2 * math.factorial(na - 2))\n",
    "        \n",
    "        tot = 0\n",
    "        for pair in itertools.combinations(list(p.keys()), 2): \n",
    "            tot += abs(p[pair[0]] - p[pair[1]])\n",
    "        \n",
    "        return float(tot/na)\n",
    "\n",
    "    \n",
    "    #Get the protected attribute columns \n",
    "    assert len(patts.keys()) == 1\n",
    "    protected = [patts[cat] for cat in patts.keys()][0]\n",
    "    \n",
    "    probs = {}   \n",
    "    #Compute p(y_hat=1 | a, y)  Va  (demographic parity)\n",
    "\n",
    "    for aval in protected: \n",
    "        if '_DUMmY' in aval:\n",
    "            subpop = (d[[a for a in protected if '_DUMmY' not in a]] == 0).all(1).values.squeeze()\n",
    "        else:\n",
    "            subpop = (d[aval] == 1).values.squeeze()\n",
    "            \n",
    "        if ftype == 'DP': \n",
    "            probs[aval] = pred[subpop].sum() / len(pred[subpop])\n",
    "        \n",
    "        elif ftype == 'EOP':\n",
    "            probs[aval] = pred[subpop & (ground == 1).squeeze()].sum() / len(pred[subpop & (ground == 1).squeeze()])\n",
    "        \n",
    "        elif ftype == 'CAL':\n",
    "            probs[aval] = ground[subpop & (pred == 1).squeeze()].sum() / len(ground[subpop])\n",
    "    \n",
    "    return avg_diff_scores(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_subset(df, subset):\n",
    "    '''Get a subset of df rows whose columns specified in subset equal their respective values\n",
    "    :param df: Dataframe (pandas)\n",
    "    :param subset: Series of col_name:value pairs (pandas series)\n",
    "    '''\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col, val in pd.Series.iteritems(subset):\n",
    "        new_df = new_df[new_df[col] == val]\n",
    "    return new_df\n",
    "\n",
    "def get_dset_fname(dset, b):\n",
    "    if dset == 'adult':\n",
    "        datafname = join(join(b, 'data'), 'adult.csv')\n",
    "    elif dset == 'german':\n",
    "        datafname = join(join(b, 'data'), 'germanCredit.csv')\n",
    "    else:\n",
    "        raise Exception('Dataset unimplemented')\n",
    "    \n",
    "    return datafname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_existing_results(allcols, ags):\n",
    "    ''' param allcols: A list of the features to be included\n",
    "        param ags: A list of paramdfs for each algorithm'''\n",
    "    add = pd.DataFrame()\n",
    "    for param_df in ags: \n",
    "        if add.empty:\n",
    "            add = param_df[allcols]\n",
    "        else:\n",
    "            add = add.append(param_df[allcols], ignore_index=True)\n",
    "    \n",
    "    uniq = np.logical_not(add.duplicated())\n",
    "    return add[uniq]\n",
    "    \n",
    "    \n",
    "def generate_results(fixed, compared): \n",
    "    '''\n",
    "    :param fixed: A list of tuples (pname, pval) that are fixed across exps\n",
    "    :param compared: A dictionary of pname:full range of possible values in experiment ''' \n",
    "    \n",
    "    fixed_results = pd.Series([np.nan]*len(fixed), index=[f[0] for f in fixed]) #  , index=fixed_cols) \n",
    "    for f in fixed:\n",
    "        fixed_results[f[0]] = f[1]\n",
    "    \n",
    "    #Set Up the Results Dataframe \n",
    "    compared_results = pd.DataFrame(itertools.product(*[compared[cat] for cat in compared]))\n",
    "    compared_results.columns = list(compared.keys())\n",
    "    \n",
    "    #Set up the results \n",
    "    results = fixed_results.to_frame().T\n",
    "    results['key'] = 0 \n",
    "    compared_results['key'] = 0\n",
    "    results = results.merge(compared_results, on='key', how='inner')\n",
    "    results.drop('key', axis='columns', inplace=True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_var_train_val(mod, ad, ay, seed, val):\n",
    "    if val != '-1': \n",
    "        val = float(val)\n",
    "        assert 0.0 < float(val) < 1.0\n",
    "        train_data, train_y_all = mod.get_traindata(ad, ay, float(val), int(seed))\n",
    "        val_data, val_y_all = mod.get_valdata(ad, ay, float(val), int(seed))\n",
    "    else:\n",
    "        train_data, train_y_all = ad, ay\n",
    "        val_data, val_y_all = pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "    return train_data, train_y_all, val_data, val_y_all\n",
    "        \n",
    "def split_train_val(ad, ay, atts, val):\n",
    "    assert (type(val) == str) and ('[' not in val)\n",
    "    \n",
    "    e_store = eproc.get_environments(ad, atts)\n",
    "    if val != '-1':\n",
    "        val_ein = e_store.pop(tuple([val])) \n",
    "        train_data, train_y_all = ad[np.logical_not(val_ein.values)], ay[np.logical_not(val_ein.values)]\n",
    "        val_data, val_y_all = ad[val_ein.values], ay[val_ein.values]\n",
    "    else:\n",
    "        train_data, train_y_all = ad, ay\n",
    "        val_data, val_y_all = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    return train_data, train_y_all, val_data, val_y_all\n",
    "    \n",
    "\n",
    "def compute_results(algos, resdf, orig_cols, from_scratch=False):\n",
    "    reddata = -1\n",
    "    loss_types = ['ACC']\n",
    "    fairness_types = ['DP', 'EOP', 'CAL']\n",
    "    sens_atts = {'adult':['race', 'gender', 'relationship'], \\\n",
    "                 'german':['Personal']}     \n",
    "    \n",
    "    #Get All The Results Columns of Interest: \n",
    "    res_cols = []\n",
    "    for al in algos.keys():\n",
    "        for m in ['train', 'val']:\n",
    "            for l in loss_types:\n",
    "                res_cols.append('{}-{}_error-{}'.format(al, m, l))\n",
    "            for f in fairness_types:\n",
    "                res_cols.append('{}-{}_fairness-{}'.format(al, m, f)) \n",
    "    for col in res_cols:  #Add cols to resultsdf\n",
    "        if (col not in list(resdf.columns)) or from_scratch:\n",
    "            resdf[col] = np.nan\n",
    "    \n",
    "    \n",
    "    for al in algos.keys():  #Enumerate through algos     \n",
    "        for resid, row in resdf.iterrows():\n",
    "            algo_rescols = [c for c in res_cols if al in c]\n",
    "            if row[algo_rescols].isnull().all():   #Check if merics for row already been computed \n",
    "                \n",
    "                #Get entry of real dataset correpsonding to row \n",
    "                rel = df_subset(algos[al]['params'], row[orig_cols])  #Get row-associated entry in param dframe\n",
    "                assert rel.shape[0] <= 1 #Guarentee just one (Excluding multi-index mappings)\n",
    "                if rel.shape[0] == 0: \n",
    "                    continue\n",
    "                \n",
    "                alldata, all_y_all, d_atts = dp.data_loader(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                              proc_fteng(row['Fteng']), \\\n",
    "                                                              dsize=reddata, \\\n",
    "                                                              bin=row['Bin'])\n",
    "                #Split data\n",
    "                train_data, train_y_all, val_data, val_y_all = split_train_val(alldata, all_y_all, \\\n",
    "                                                                               {row['Envs']:d_atts[row['Envs']]}, \\\n",
    "                                                                               row['Val'])  \n",
    "                #Compute Predictions  \n",
    "                if al == 'icp':\n",
    "                    model = models.InvariantCausalPrediction() \n",
    "                    learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'coeffs'])]\n",
    "                    \n",
    "                       \n",
    "                    train_predictions = model.predict(train_data, *learned_model)\n",
    "                    val_predictions = model.predict(val_data, *learned_model)\n",
    "\n",
    "                elif al == 'irm':\n",
    "                    model = models.InvariantRiskMinimization()\n",
    "                    try:\n",
    "                        learned_model = [torch.load(rel.loc[rel.index[0], 'phi'])]\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "       \n",
    "                    train_predictions = model.predict(train_data.values, *learned_model)\n",
    "                    val_predictions = model.predict(val_data.values, *learned_model)                \n",
    "\n",
    "                elif al == 'linreg':\n",
    "                    model = models.Linear()\n",
    "                    learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'linregressors'])]\n",
    "                    \n",
    "                    #Split data and predict\n",
    "                    train_data, train_y_all, val_data, val_y_all = split_var_train_val(alldata, \\\n",
    "                                                                                       all_y_all, row['Seed'], \\\n",
    "                                                                                       row['Val'])     \n",
    "                    train_predictions = model.predict(train_data, *learned_model)   \n",
    "                    val_predictions = model.predict(val_data, *learned_model)  \n",
    "\n",
    "                    \n",
    "                #Compute Metrics on Predictions \n",
    "                for ftype in fairness_types:\n",
    "                    for ltype in loss_types:\n",
    "                        for r in [['train', train_predictions, train_y_all, train_data], ['val', val_predictions, val_y_all, val_data]] :\n",
    "                            m, predictions, y_all, data = r[0], r[1], r[2], r[3]\n",
    "                        \n",
    "                            #Manage special case \n",
    "                            if predictions.empty:\n",
    "                                resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = 'NA'\n",
    "                                resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = 'NA'\n",
    "\n",
    "                            else:\n",
    "                                error = compute_loss(predictions.values, y_all.values, ltype=ltype)\n",
    "                                full_fair= ''\n",
    "                                for s in sens_atts[row['Dataset']]:\n",
    "                                    fairness =  fairness_dp(pred_binarize(predictions.values), y_all.values,\\\n",
    "                                                            data, {s:d_atts[s]}, ftype=ftype)\n",
    "                                    full_fair = full_fair + ' {0}:{1:.3f} \\n'.format(s, fairness)\n",
    "\n",
    "\n",
    "                                #Save computed values to resdf \n",
    "                                resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = error\n",
    "                                resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = full_fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Invariance Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invar_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Eq_Estrat', -1]] \n",
    "\n",
    "invar_COMPARED =  {'Envs':['workclass', 'native-country'], \\\n",
    "                   'Seed':[147, 256, 304],\n",
    "                   'Fteng':['1', '12'], \\\n",
    "                   'Bin':[1]}\n",
    "\n",
    "invar_orig_cols = [a[0] for a in invar_FIXED] + list(invar_COMPARED.keys()) + ['Val']\n",
    "invar_results = generate_all_existing_results(invar_orig_cols, \\\n",
    "                                             [invariance_algos[a]['params'] for a in list(invariance_algos.keys())])    \n",
    "#invar_results = generate_results(invar_FIXED, invar_COMPARED)\n",
    "\n",
    "invar_results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(invariance_algos, invar_results, invar_orig_cols, from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 4000\n",
    "invar_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Non-Invariance Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Bin', 1]] \n",
    "\n",
    "var_COMPARED =  {'Fteng':['1', '12'], \\\n",
    "                 'Seed':[147, 256, 304]}\n",
    "\n",
    "var_orig_cols = [a[0] for a in var_FIXED] + list(var_COMPARED.keys()) + ['Val']\n",
    "var_results = generate_all_existing_results(var_orig_cols, \\\n",
    "                                             [non_invariance_algos[a]['params'] for a in list(non_invariance_algos.keys())]) \n",
    "# var_results = generate_results(var_FIXED, var_COMPARED)\n",
    "\n",
    "var_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(non_invariance_algos, var_results, var_orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "var_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save To Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_resdir = 'latex_results'\n",
    "latex_fname = '0602_validation_invar.xlsx'\n",
    "invar_results.to_excel(os.path.join(latex_resdir, latex_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
