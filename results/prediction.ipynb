{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import os \n",
    "from os.path import join\n",
    "import shutil\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [6, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "basedir = '/Users/RobertAdragna/Documents/School/Fourth_Year/ESC499-Thesis/codebases/causal_discovery'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import data_processing as dp \n",
    "import environment_processing as eproc \n",
    "import models \n",
    "from utils import proc_fteng, make_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '0610_baseline_adultgerman/hyperparam_tuning'\n",
    "algo = 'logreg'\n",
    "algo_params = pd.read_pickle(join(join(join(os.getcwd(), res_dir), algo), '{}_paramfile.pkl'.format(algo)))\n",
    "\n",
    "\n",
    "algo_pdir = join(join(join(os.getcwd(), res_dir), algo), 'processed_results')\n",
    "if not os.path.exists(algo_pdir):\n",
    "    raise Exception('Directory has not yet been processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_binarize(v):\n",
    "    '''Convert all values to 0 if <0.5, 1 otherwise'''\n",
    "    def thresh(x):\n",
    "        if (x >= 0.5): return 1 \n",
    "        else: return 0\n",
    "    print(v.shape)\n",
    "    return np.apply_along_axis(thresh, 1, v)\n",
    "    \n",
    "        \n",
    "def compute_loss(pred, ground, ltype='MSE'):\n",
    "    '''Compute loss between two prediction vectors'''\n",
    "\n",
    "    \n",
    "    if ltype == 'MSE':\n",
    "        return F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    if ltype == 'ACC':\n",
    "        pred = pred_binarize(pred) \n",
    "        return 1 - F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    \n",
    "def fairness_dp(pred, ground, d, patts, ftype='DP'):\n",
    "    '''Compute demographic aparity wrt data\n",
    "    :param pred: vector, binary entries (np[float])\n",
    "    :param ground: vector, binary entries (np[float])\n",
    "    :param d: dataset (pandas df)\n",
    "    :param patt: datts dict {cat:[all orig columns]}'''\n",
    "    \n",
    "    def avg_diff_scores(p):\n",
    "        ''' Given a dictionary of scores for different sensitive attributes p.keys, \\ \n",
    "            return the average difference between these values '''\n",
    "        na = len(p.keys())\n",
    "        if na <= 1:  #Error checking\n",
    "            return np.nan\n",
    "        \n",
    "        n_combos = math.factorial(na)/ (2 * math.factorial(na - 2))\n",
    "        \n",
    "        tot = 0\n",
    "        for pair in itertools.combinations(list(p.keys()), 2): \n",
    "            tot += abs(p[pair[0]] - p[pair[1]])\n",
    "        \n",
    "        return float(tot/na)\n",
    "\n",
    "    \n",
    "    #Get the protected attribute columns \n",
    "    assert len(patts.keys()) == 1\n",
    "    protected = [patts[cat] for cat in patts.keys()][0]\n",
    "    \n",
    "    probs = {}   \n",
    "    #Compute p(y_hat=1 | a, y)  Va  (demographic parity)\n",
    "\n",
    "    for aval in protected: \n",
    "        if '_DUMmY' in aval:\n",
    "            subpop = (d[[a for a in protected if '_DUMmY' not in a]] == 0).all(1).values.squeeze()\n",
    "        else:\n",
    "            subpop = (d[aval] == 1).values.squeeze()\n",
    "        \n",
    "        #Make sure that there are samples in the group of interest \n",
    "        if (subpop.sum() == 0) or ((subpop & (ground == 1).squeeze()).sum() == 0):\n",
    "            continue\n",
    "        \n",
    "        #Compute fairness\n",
    "        if ftype == 'DP': \n",
    "            probs[aval] = pred[subpop].sum() / len(pred[subpop])\n",
    "        \n",
    "        elif ftype == 'EOP':\n",
    "            probs[aval] = pred[subpop & (ground == 1).squeeze()].sum() / len(pred[subpop & (ground == 1).squeeze()])\n",
    "        \n",
    "        elif ftype == 'CAL':\n",
    "            probs[aval] = ground[subpop & (pred == 1).squeeze()].sum() / len(ground[subpop])\n",
    "\n",
    "    return avg_diff_scores(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_subset(df, subset):\n",
    "    '''Get a subset of df rows whose columns specified in subset equal their respective values\n",
    "    :param df: Dataframe (pandas)\n",
    "    :param subset: Series of col_name:value pairs (pandas series)\n",
    "    '''\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col, val in pd.Series.iteritems(subset):\n",
    "        new_df = new_df[new_df[col] == val]\n",
    "    return new_df\n",
    "\n",
    "def get_dset_fname(dset, b):\n",
    "    if dset == 'adult':\n",
    "        datafname = join(join(b, 'data'), 'adult.csv')\n",
    "    elif dset == 'german':\n",
    "        datafname = join(join(b, 'data'), 'germanCredit.csv')\n",
    "    else:\n",
    "        raise Exception('Dataset unimplemented')\n",
    "    \n",
    "    return datafname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_existing_results(allcols, ags):\n",
    "    ''' param allcols: A list of the features to be included\n",
    "        param ags: A list of paramdfs for each algorithm'''\n",
    "    add = pd.DataFrame()\n",
    "    for param_df in ags: \n",
    "        if add.empty:\n",
    "            add = param_df[allcols]\n",
    "        else:\n",
    "            add = add.append(param_df[allcols], ignore_index=True)\n",
    "    \n",
    "    uniq = np.logical_not(add.duplicated())\n",
    "    return add[uniq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(al, al_params, old_resdf, orig_cols):\n",
    "    reddata = -1\n",
    "    loss_types = ['ACC']\n",
    "    fairness_types = ['DP', 'EOP', 'CAL']\n",
    "    sens_atts = {'adult':['race'], \\\n",
    "                 'german':['Personal']}     \n",
    "    \n",
    "    resdf = old_resdf.copy()\n",
    "    \n",
    "    #Get All The Results Columns of Interest: \n",
    "    res_cols = []\n",
    "    for m in ['train', 'test']:\n",
    "        for l in loss_types:\n",
    "            res_cols.append('{}-{}_error-{}'.format(al, m, l))\n",
    "        for f in fairness_types:\n",
    "            res_cols.append('{}-{}_fairness-{}'.format(al, m, f)) \n",
    "    for col in res_cols:  #Add cols to resultsdf\n",
    "        resdf[col] = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    for resid, row in resdf.iterrows():\n",
    "        algo_rescols = [c for c in res_cols if al in c]\n",
    "        if row[algo_rescols].isnull().all():   #Check if merics for row already been computed \n",
    "\n",
    "            #Get entry of real dataset correpsonding to row \n",
    "            rel = df_subset(al_params, row[orig_cols])  #Get row-associated entry in param dframe\n",
    "            assert rel.shape[0] <= 1 #Guarentee just one (Excluding multi-index mappings)\n",
    "            if rel.shape[0] == 0: \n",
    "                continue\n",
    "\n",
    "            alldata, all_y_all, d_atts = dp.data_loader(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                          proc_fteng(row['Fteng']), \\\n",
    "                                                          dsize=reddata, \\\n",
    "                                                          bin=row['Bin'])\n",
    "            #Split data\n",
    "            train_data, train_y_all, d_atts, _, _, test_data, test_y_all = dp.train_val_test_split(\\\n",
    "                                                                      alldata, all_y_all, d_atts, test=row['TestSet'])\n",
    "\n",
    "            #Compute Predictions  \n",
    "            if al == 'icp':\n",
    "                model = models.InvariantCausalPrediction() \n",
    "                learned_model = [pd.read_pickle(join(res_dir, rel.loc[rel.index[0], 'coeffs']))]\n",
    "\n",
    "\n",
    "                train_predictions = model.predict(train_data, *learned_model)\n",
    "                test_predictions = model.predict(test_data, *learned_model)\n",
    "\n",
    "            elif (al == 'irm') or (al == 'linear_irm'):\n",
    "                if (al == 'irm'):\n",
    "                    model = models.InvariantRiskMinimization()\n",
    "                    try:\n",
    "                        learned_model = [torch.load(join(res_dir, rel.loc[rel.index[0], 'phi']))]\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "                elif (al == 'linear_irm'):\n",
    "                    model = models.LinearInvariantRiskMinimization()\n",
    "                    try:\n",
    "                        learned_model = [torch.load(join(res_dir, rel.loc[rel.index[0], 'phi']))]\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "                train_predictions = model.predict(train_data.values, *learned_model, hid_layers=200)\n",
    "                test_predictions = model.predict(test_data.values, *learned_model, hid_layers=200)\n",
    "\n",
    "            elif al == 'linreg':\n",
    "                model = models.Linear()\n",
    "                learned_model = [pd.read_pickle(join(res_dir, rel.loc[rel.index[0], 'regressors']))]\n",
    "\n",
    "                train_predictions = model.predict(train_data, *learned_model)   \n",
    "                test_predictions = model.predict(test_data, *learned_model)  \n",
    "\n",
    "            elif al == 'logreg':\n",
    "                model = models.LogisticReg()\n",
    "                learned_model = [pd.read_pickle(join(res_dir, rel.loc[rel.index[0], 'regressors']))]\n",
    "\n",
    "                train_predictions = model.predict(train_data, *learned_model)   \n",
    "                test_predictions = model.predict(test_data, *learned_model) \n",
    "                \n",
    "            elif (al == 'mlp'):\n",
    "                model = models.MLP()\n",
    "                learned_model = [torch.load(join(res_dir, rel.loc[rel.index[0], 'weights']))]\n",
    "                train_predictions = model.predict(train_data.values, *learned_model, hid_layers=50)\n",
    "                test_predictions = model.predict(test_data.values, *learned_model, hid_layers=50)  \n",
    "\n",
    "            elif al == 'constant':\n",
    "                model = models.Constant()\n",
    "\n",
    "                train_predictions = model.predict(train_data)   \n",
    "                test_predictions = model.predict(test_data) \n",
    "\n",
    "\n",
    "                \n",
    "            #Compute Metrics on Predictions \n",
    "            for ftype in fairness_types:\n",
    "                for ltype in loss_types:\n",
    "                    for r in [['train', train_predictions, train_y_all, train_data], ['test', test_predictions, test_y_all, test_data]] :\n",
    "                        m, predictions, y_all, data = r[0], r[1], r[2], r[3]\n",
    "\n",
    "                        #Manage special case \n",
    "                        if predictions.empty:\n",
    "                            resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = 'NA'\n",
    "                            resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = 'NA'\n",
    "\n",
    "                        else:\n",
    "                            error = compute_loss(predictions.values, y_all.values, ltype=ltype)\n",
    "                            full_fair = 0\n",
    "                            for s in sens_atts[row['Dataset']]:\n",
    "                                fairness =  fairness_dp(pred_binarize(predictions.values), y_all.values,\\\n",
    "                                                            data, {s:d_atts[s]}, ftype=ftype)\n",
    "\n",
    "                                if not np.isnan(fairness):\n",
    "                                    full_fair += fairness/len(sens_atts[row['Dataset']])\n",
    "\n",
    "\n",
    "                            #Save computed values to resdf \n",
    "                            resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = error\n",
    "                            resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = full_fair\n",
    "\n",
    "    return resdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    alldata, all_y_all, d_atts = dp.data_loader(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                              proc_fteng(row['Fteng']), \\\n",
    "                                                              dsize=-1, \\\n",
    "                                                              bin=row['Bin'])\n",
    "    assert 'Val' not in row.index\n",
    "    train_data, train_labels, d_atts, val_data, val_labels, test_data, test_labels = \\\n",
    "         dp.train_val_test_split(alldata, all_y_all, d_atts, val=0.2, test=row['TestSet'], seed=row['Seed'])\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "        \n",
    "\n",
    "def compute_hyperparameters(true_resdf):\n",
    "    def compute_irm_loss(model, logits, labels, pen_reg):\n",
    "        logits, labels = make_tensor(logits.values), make_tensor(labels.values)\n",
    "        loss = model.mean_nll(logits, labels)\n",
    "        pen = model.penalty(logits, labels)\n",
    "        return (loss + (pen_reg * pen)).detach().numpy()\n",
    "    def compute_linreg_loss(logits, labels, weight, lam):\n",
    "        return ((logits - labels) ** 2).mean() + (weight * lam)\n",
    "    def compute_logreg_loss(logits, labels, weight, lam):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels).detach().numpy() + (weight * lam)\n",
    "        \n",
    "    def compute_mlp_loss(logits, labels, weight_pen, lam):\n",
    "        logits, labels = make_tensor(logits.values), make_tensor(labels.values)\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels).detach().numpy() + (weight_pen * lam).detach().numpy()\n",
    "    \n",
    "    \n",
    "    resdf = true_resdf.copy(deep=True)\n",
    "    resdf['training_loss'] = np.nan\n",
    "    resdf['validation_loss'] = np.nan\n",
    "                                                                               \n",
    "    for resid, row in resdf.iterrows():\n",
    "        \n",
    "        #Load the data\n",
    "        train_data, train_labels, val_data, val_labels, _, _ = split_data(row)\n",
    "        \n",
    "        if (row['Algo'] == 'irm') or (row['Algo'] == 'linear_irm') :                                                                         \n",
    "            #Load the model\n",
    "            if (row['Algo'] == 'irm'):\n",
    "                try:\n",
    "                    src = models.InvariantRiskMinimization()\n",
    "                    params = torch.load(join(res_dir, row['phi']))\n",
    "                except:\n",
    "                    resdf.drop(resid)\n",
    "                    continue\n",
    "                                      \n",
    "            elif (row['Algo'] == 'linear_irm'):\n",
    "                try:\n",
    "                    src = models.LinearInvariantRiskMinimization()\n",
    "                    params = torch.load(join(res_dir, row['phi']))\n",
    "                except:\n",
    "                    resdf.drop(resid)\n",
    "                    continue\n",
    "                     \n",
    "            train_logits = src.predict(train_data.values, params, hid_layers=row['HidLayers'])\n",
    "            train_loss = compute_irm_loss(src, train_logits, \\\n",
    "                                      train_labels, row['PenWeight'])\n",
    "            val_logits = src.predict(val_data.values, params, hid_layers=row['HidLayers'])\n",
    "            val_loss = compute_irm_loss(src, val_logits, \\\n",
    "                                      val_labels, row['PenWeight'])\n",
    "            \n",
    "        elif (row['Algo'] == 'linreg'):\n",
    "            src = models.Linear()\n",
    "            coeffs = pd.read_pickle(join(res_dir, row['regressors']))\n",
    "            weight = src.get_weight_norm(coeffs)\n",
    "            \n",
    "            train_logits = src.predict(train_data, coeffs)\n",
    "            train_loss = compute_linreg_loss(train_logits.values, train_labels.values, weight, row['Reg'])\n",
    "            val_logits = src.predict(val_data, coeffs)\n",
    "            val_loss = compute_linreg_loss(val_logits.values, val_labels.values, weight, row['Reg'])\n",
    "              \n",
    "        elif (row['Algo'] == 'logreg'):\n",
    "            src = models.LogisticReg()\n",
    "            coeffs = pd.read_pickle(join(res_dir, row['regressors']))\n",
    "            weight = src.get_weight_norm(coeffs)\n",
    "            \n",
    "            train_logits = src.predict(train_data, coeffs)\n",
    "            train_loss = compute_linreg_loss(train_logits.values, train_labels.values, weight, row['Reg'])\n",
    "            val_logits = src.predict(val_data, coeffs)\n",
    "            val_loss = compute_linreg_loss(val_logits.values, val_labels.values, weight, row['Reg'])\n",
    "        \n",
    "        elif (row['Algo'] == 'mlp'):\n",
    "            src = models.MLP()\n",
    "            weights = torch.load(join(res_dir, row['weights']))\n",
    "            w_norm = src.get_weight_norm(weights, dsize=train_data.shape[1], hid_layers=row['HidLayers'])\n",
    "#             import pdb; pdb.set_trace()\n",
    "            train_logits = src.predict(train_data.values, weights, hid_layers=row['HidLayers'])\n",
    "            train_loss = compute_mlp_loss(train_logits, train_labels, w_norm, row['L2_WeightPen'])\n",
    "            val_logits = src.predict(val_data.values, weights, hid_layers=row['HidLayers'])\n",
    "            val_loss = compute_mlp_loss(val_logits, val_labels, w_norm, row['L2_WeightPen'])\n",
    "        \n",
    "        resdf.loc[resid, 'training_loss'] = train_loss    \n",
    "        resdf.loc[resid, 'validation_loss'] = val_loss\n",
    "    return resdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = compute_hyperparameters(algo_params)\n",
    "\n",
    "if (algo == 'irm') or (algo == 'linear-irm'):\n",
    "    res.drop('phi', axis=1, inplace=True)\n",
    "elif (algo == 'linreg') or (algo == 'logreg'):\n",
    "    res.drop('regressors', axis=1, inplace=True)\n",
    "elif (algo == 'mlp'):\n",
    "    res.drop('weights', axis=1, inplace=True)\n",
    "else: \n",
    "    raise Exception('Unimplemented Algo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = res[res['Dataset'] == 'adult']\n",
    "at = adult.sort_values(by=['training_loss'])\n",
    "av = adult.sort_values(by=['validation_loss'])\n",
    "\n",
    "german = res[res['Dataset'] == 'german']\n",
    "gt = german.sort_values(by=['training_loss'])\n",
    "gv = german.sort_values(by=['validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Algo', 'Fteng', 'Dataset', 'ReduceDsize', 'Bin'] #['Algo', 'Fteng', 'Dataset', 'ReduceDsize', 'Bin', 'Eq_Estrat', 'Envs']  # ['Algo', 'Fteng', 'Dataset', 'ReduceDsize', 'Bin']\n",
    "view_cols = [\"TestSet\", 'LR', 'N_Iterations', 'L2_WeightPen', 'HidLayers']  # [\"TestSet\", 'Reg']\n",
    "\n",
    "tmp = gv.drop(drop_cols, axis=1)\n",
    "tmp = gv.groupby([\"TestSet\", 'N_Iterations'])[['training_loss', 'validation_loss']].mean()\n",
    "tmp.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo in ['irm', 'linear-irm', 'icp']:\n",
    "    orig_cols = ['Dataset', 'ReduceDsize', 'Eq_Estrat', 'Envs', 'Seed', 'Fteng', 'Bin', 'TestSet']\n",
    "elif algo in ['linreg', 'logreg', 'mlp', 'constant']:\n",
    "    orig_cols = ['Dataset', 'ReduceDsize', 'Seed', 'Fteng', 'Bin', 'TestSet']\n",
    "else:\n",
    "    raise Exception('Algo not implemented')\n",
    "\n",
    "orig_results = generate_all_existing_results(orig_cols, [algo_params])\n",
    "orig_results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compute_results(algo, algo_params, orig_results, orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 4000\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save To Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_fname = '{}_results.xlsx'.format(algo)\n",
    "results.to_excel(join(join(join(join(os.getcwd(), res_dir), algo), 'analysis'), excel_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
