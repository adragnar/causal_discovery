{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import os \n",
    "from os.path import join\n",
    "import shutil\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import json\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = [6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "basedir = '/Users/RobertAdragna/Documents/School/Fourth_Year/ESC499-Thesis/codebases/causal_discovery'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import data_processing as dp \n",
    "import environment_processing as eproc \n",
    "import models \n",
    "from utils import proc_fteng, make_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '0610_linearirm'\n",
    "invariance_algos = {'linear_irm':{}}   #'icp':{},\n",
    "non_invariance_algos = {}\n",
    "\n",
    "for als in [invariance_algos, non_invariance_algos]:\n",
    "    for al in als.keys(): \n",
    "        als[al]['expdir'] = join(join(join(os.getcwd(), res_dir), al), 'causal_discovery')\n",
    "        als[al]['processed_dir'] = join(join(join(os.getcwd(), res_dir), al), 'processed_results')\n",
    "        als[al]['params'] = pd.read_pickle(join(join(join(os.getcwd(), res_dir), al), '{}_paramfile.pkl'.format(al)))\n",
    "\n",
    "        if not os.path.exists(als[al]['processed_dir']):\n",
    "            raise Exception('Directory has not yet been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariance_algos['linear_irm']['params'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_binarize(v):\n",
    "    '''Convert all values to 0 if <0.5, 1 otherwise'''\n",
    "    def thresh(x):\n",
    "        if (x >= 0.5): return 1 \n",
    "        else: return 0\n",
    "    print(v.shape)\n",
    "    return np.apply_along_axis(thresh, 1, v)\n",
    "    \n",
    "        \n",
    "def compute_loss(pred, ground, ltype='MSE'):\n",
    "    '''Compute loss between two prediction vectors'''\n",
    "\n",
    "    \n",
    "    if ltype == 'MSE':\n",
    "        return F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    if ltype == 'ACC':\n",
    "        pred = pred_binarize(pred) \n",
    "        return 1 - F.mse_loss(torch.tensor(pred).float(), torch.tensor(ground).float()).numpy()\n",
    "    \n",
    "def fairness_dp(pred, ground, d, patts, ftype='DP'):\n",
    "    '''Compute demographic aparity wrt data\n",
    "    :param pred: vector, binary entries (np[float])\n",
    "    :param ground: vector, binary entries (np[float])\n",
    "    :param d: dataset (pandas df)\n",
    "    :param patt: datts dict {cat:[all orig columns]}'''\n",
    "    \n",
    "    def avg_diff_scores(p):\n",
    "        ''' Given a dictionary of scores for different sensitive attributes p.keys, \\ \n",
    "            return the average difference between these values '''\n",
    "        na = len(p.keys())\n",
    "        if na <= 1:  #Error checking\n",
    "            return np.nan\n",
    "        \n",
    "        n_combos = math.factorial(na)/ (2 * math.factorial(na - 2))\n",
    "        \n",
    "        tot = 0\n",
    "        for pair in itertools.combinations(list(p.keys()), 2): \n",
    "            tot += abs(p[pair[0]] - p[pair[1]])\n",
    "        \n",
    "        return float(tot/na)\n",
    "\n",
    "    \n",
    "    #Get the protected attribute columns \n",
    "    assert len(patts.keys()) == 1\n",
    "    protected = [patts[cat] for cat in patts.keys()][0]\n",
    "    \n",
    "    probs = {}   \n",
    "    #Compute p(y_hat=1 | a, y)  Va  (demographic parity)\n",
    "\n",
    "    for aval in protected: \n",
    "        if '_DUMmY' in aval:\n",
    "            subpop = (d[[a for a in protected if '_DUMmY' not in a]] == 0).all(1).values.squeeze()\n",
    "        else:\n",
    "            subpop = (d[aval] == 1).values.squeeze()\n",
    "        \n",
    "        #Make sure that there are samples in the group of interest \n",
    "        if (subpop.sum() == 0) or ((subpop & (ground == 1).squeeze()).sum() == 0):\n",
    "            continue\n",
    "        \n",
    "        #Compute fairness\n",
    "        if ftype == 'DP': \n",
    "            probs[aval] = pred[subpop].sum() / len(pred[subpop])\n",
    "        \n",
    "        elif ftype == 'EOP':\n",
    "            probs[aval] = pred[subpop & (ground == 1).squeeze()].sum() / len(pred[subpop & (ground == 1).squeeze()])\n",
    "        \n",
    "        elif ftype == 'CAL':\n",
    "            probs[aval] = ground[subpop & (pred == 1).squeeze()].sum() / len(ground[subpop])\n",
    "\n",
    "    return avg_diff_scores(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_subset(df, subset):\n",
    "    '''Get a subset of df rows whose columns specified in subset equal their respective values\n",
    "    :param df: Dataframe (pandas)\n",
    "    :param subset: Series of col_name:value pairs (pandas series)\n",
    "    '''\n",
    "    new_df = df.copy(deep=True)\n",
    "    for col, val in pd.Series.iteritems(subset):\n",
    "        new_df = new_df[new_df[col] == val]\n",
    "    return new_df\n",
    "\n",
    "def get_dset_fname(dset, b):\n",
    "    if dset == 'adult':\n",
    "        datafname = join(join(b, 'data'), 'adult.csv')\n",
    "    elif dset == 'german':\n",
    "        datafname = join(join(b, 'data'), 'germanCredit.csv')\n",
    "    else:\n",
    "        raise Exception('Dataset unimplemented')\n",
    "    \n",
    "    return datafname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_existing_results(allcols, ags):\n",
    "    ''' param allcols: A list of the features to be included\n",
    "        param ags: A list of paramdfs for each algorithm'''\n",
    "    add = pd.DataFrame()\n",
    "    for param_df in ags: \n",
    "        if add.empty:\n",
    "            add = param_df[allcols]\n",
    "        else:\n",
    "            add = add.append(param_df[allcols], ignore_index=True)\n",
    "    \n",
    "    uniq = np.logical_not(add.duplicated())\n",
    "    return add[uniq]\n",
    "    \n",
    "    \n",
    "def generate_results(fixed, compared): \n",
    "    '''\n",
    "    :param fixed: A list of tuples (pname, pval) that are fixed across exps\n",
    "    :param compared: A dictionary of pname:full range of possible values in experiment ''' \n",
    "    \n",
    "    fixed_results = pd.Series([np.nan]*len(fixed), index=[f[0] for f in fixed]) #  , index=fixed_cols) \n",
    "    for f in fixed:\n",
    "        fixed_results[f[0]] = f[1]\n",
    "    \n",
    "    #Set Up the Results Dataframe \n",
    "    compared_results = pd.DataFrame(itertools.product(*[compared[cat] for cat in compared]))\n",
    "    compared_results.columns = list(compared.keys())\n",
    "    \n",
    "    #Set up the results \n",
    "    results = fixed_results.to_frame().T\n",
    "    results['key'] = 0 \n",
    "    compared_results['key'] = 0\n",
    "    results = results.merge(compared_results, on='key', how='inner')\n",
    "    results.drop('key', axis='columns', inplace=True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(algos, resdf, orig_cols, from_scratch=False):\n",
    "    reddata = -1\n",
    "    loss_types = ['ACC']\n",
    "    fairness_types = ['DP', 'EOP', 'CAL']\n",
    "    sens_atts = {'adult':['race', 'gender', 'relationship'], \\\n",
    "                 'german':['Personal']}     \n",
    "    \n",
    "    #Get All The Results Columns of Interest: \n",
    "    res_cols = []\n",
    "    for al in algos.keys():\n",
    "        for m in ['train', 'test']:\n",
    "            for l in loss_types:\n",
    "                res_cols.append('{}-{}_error-{}'.format(al, m, l))\n",
    "            for f in fairness_types:\n",
    "                res_cols.append('{}-{}_fairness-{}'.format(al, m, f)) \n",
    "    for col in res_cols:  #Add cols to resultsdf\n",
    "        if (col not in list(resdf.columns)) or from_scratch:\n",
    "            resdf[col] = np.nan\n",
    "    \n",
    "    \n",
    "    for al in algos.keys():  #Enumerate through algos     \n",
    "        for resid, row in resdf.iterrows():\n",
    "            algo_rescols = [c for c in res_cols if al in c]\n",
    "            if row[algo_rescols].isnull().all():   #Check if merics for row already been computed \n",
    "                \n",
    "                #Get entry of real dataset correpsonding to row \n",
    "                rel = df_subset(algos[al]['params'], row[orig_cols])  #Get row-associated entry in param dframe\n",
    "                assert rel.shape[0] <= 1 #Guarentee just one (Excluding multi-index mappings)\n",
    "                if rel.shape[0] == 0: \n",
    "                    continue\n",
    "                \n",
    "                alldata, all_y_all, d_atts = dp.data_loader(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                              proc_fteng(row['Fteng']), \\\n",
    "                                                              dsize=reddata, \\\n",
    "                                                              bin=row['Bin'])\n",
    "                #Split data\n",
    "                train_data, train_y_all, d_atts, _, _, test_data, test_y_all = dp.train_val_test_split(\\\n",
    "                                                                          alldata, all_y_all, d_atts, test=row['TestSet'])\n",
    "                \n",
    "                #Compute Predictions  \n",
    "                if al == 'icp':\n",
    "                    model = models.InvariantCausalPrediction() \n",
    "                    learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'coeffs'])]\n",
    "                    \n",
    "                       \n",
    "                    train_predictions = model.predict(train_data, *learned_model)\n",
    "                    test_predictions = model.predict(test_data, *learned_model)\n",
    "\n",
    "                elif (al == 'irm') or (al == 'linear_irm'):\n",
    "                    if (al == 'irm'):\n",
    "                        model = models.InvariantRiskMinimization()\n",
    "                        try:\n",
    "                            learned_model = [torch.load(rel.loc[rel.index[0], 'phi'])]\n",
    "                        except:\n",
    "                            import pdb; pdb.set_trace()\n",
    "                    elif (al == 'linear_irm'):\n",
    "                        model = models.LinearInvariantRiskMinimization()\n",
    "                        try:\n",
    "                            learned_model = [torch.load(rel.loc[rel.index[0], 'phi'])]\n",
    "                        except:\n",
    "                            import pdb; pdb.set_trace()\n",
    "                    \n",
    "       \n",
    "                    train_predictions = model.predict(train_data.values, *learned_model, hid_layers=200)\n",
    "                    test_predictions = model.predict(test_data.values, *learned_model, hid_layers=200)                \n",
    "\n",
    "                elif al == 'linreg':\n",
    "                    model = models.Linear()\n",
    "                    learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'regressors'])]\n",
    "                    \n",
    "                    train_predictions = model.predict(train_data, *learned_model)   \n",
    "                    test_predictions = model.predict(test_data, *learned_model)  \n",
    "                \n",
    "                elif al == 'logreg':\n",
    "                    model = models.LogisticReg()\n",
    "                    learned_model = [pd.read_pickle(rel.loc[rel.index[0], 'regressors'])]\n",
    "                    \n",
    "                    train_predictions = model.predict(train_data, *learned_model)   \n",
    "                    test_predictions = model.predict(test_data, *learned_model) \n",
    "                    \n",
    "                #Compute Metrics on Predictions \n",
    "                for ftype in fairness_types:\n",
    "                    for ltype in loss_types:\n",
    "                        for r in [['train', train_predictions, train_y_all, train_data], ['test', test_predictions, test_y_all, test_data]] :\n",
    "                            m, predictions, y_all, data = r[0], r[1], r[2], r[3]\n",
    "                        \n",
    "                            #Manage special case \n",
    "                            if predictions.empty:\n",
    "                                resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = 'NA'\n",
    "                                resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = 'NA'\n",
    "\n",
    "                            else:\n",
    "                                error = compute_loss(predictions.values, y_all.values, ltype=ltype)\n",
    "                                full_fair= ''\n",
    "                                for s in sens_atts[row['Dataset']]:\n",
    "                                    fairness =  fairness_dp(pred_binarize(predictions.values), y_all.values,\\\n",
    "                                                            data, {s:d_atts[s]}, ftype=ftype)\n",
    "                                    full_fair = full_fair + ' {0}:{1:.3f} \\n'.format(s, fairness)\n",
    "\n",
    "\n",
    "                                #Save computed values to resdf \n",
    "                                resdf.loc[resid, '{}-{}_error-{}'.format(al, m, ltype)] = error\n",
    "                                resdf.loc[resid, '{}-{}_fairness-{}'.format(al, m, ftype)] = full_fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(row):\n",
    "    alldata, all_y_all, d_atts = dp.data_loader(get_dset_fname(row['Dataset'], basedir), \\\n",
    "                                                              proc_fteng(row['Fteng']), \\\n",
    "                                                              dsize=-1, \\\n",
    "                                                              bin=row['Bin'])\n",
    "    assert 'Val' not in row.index\n",
    "    train_data, train_labels, d_atts, val_data, val_labels, test_data, test_labels = \\\n",
    "         dp.train_val_test_split(alldata, all_y_all, d_atts, val=0.2, test=row['TestSet'], seed=row['Seed'])\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "        \n",
    "\n",
    "def compute_hyperparameters(true_resdf):\n",
    "    def compute_irm_loss(model, logits, labels, pen_reg):\n",
    "        logits, labels = make_tensor(logits.values), make_tensor(labels.values)\n",
    "        loss = model.mean_nll(logits, labels)\n",
    "        pen = model.penalty(logits, labels)\n",
    "        return (loss + (pen_reg * pen)).detach().numpy()\n",
    "    def compute_linreg_loss(logits, labels, weight, lam):\n",
    "        return ((logits - labels) ** 2).mean() + (weight * lam)\n",
    "  \n",
    "    \n",
    "    \n",
    "    resdf = true_resdf.copy(deep=True)\n",
    "    resdf['training_loss'] = np.nan\n",
    "    resdf['validation_loss'] = np.nan\n",
    "                                                                               \n",
    "    for resid, row in resdf.iterrows():\n",
    "        \n",
    "        #Load the data\n",
    "        train_data, train_labels, val_data, val_labels, _, _ = split_data(row)\n",
    "        \n",
    "        if (row['Algo'] == 'irm') or (row['Algo'] == 'linear_irm') :                                                                         \n",
    "            #Load the model\n",
    "            if (row['Algo'] == 'irm'):\n",
    "                try:\n",
    "                    src = models.InvariantRiskMinimization()\n",
    "                    params = torch.load(row['phi'])\n",
    "                except:\n",
    "                    resdf.drop(resid)\n",
    "                    continue\n",
    "                                      \n",
    "            elif (row['Algo'] == 'linear_irm'):\n",
    "                try:\n",
    "                    src = models.LinearInvariantRiskMinimization()\n",
    "                    params = torch.load(row['phi'])\n",
    "                except:\n",
    "                    resdf.drop(resid)\n",
    "                    continue\n",
    "                     \n",
    "            train_logits = src.predict(train_data.values, params, hid_layers=row['HidLayers'])\n",
    "            train_loss = compute_irm_loss(src, train_logits, \\\n",
    "                                      train_labels, row['PenWeight'])\n",
    "            val_logits = src.predict(val_data.values, params, hid_layers=row['HidLayers'])\n",
    "            val_loss = compute_irm_loss(src, val_logits, \\\n",
    "                                      val_labels, row['PenWeight'])\n",
    "            \n",
    "        elif (row['Algo'] == 'linreg'):\n",
    "            src = models.Linear()\n",
    "            coeffs = pd.read_pickle(row['regressors'])\n",
    "            weight = src.get_weight_norm(coeffs)\n",
    "            \n",
    "            train_logits = src.predict(train_data, coeffs)\n",
    "            train_loss = compute_linreg_loss(train_logits.values, train_labels.values, weight, row['Reg'])\n",
    "            val_logits = src.predict(val_data, coeffs)\n",
    "            val_loss = compute_linreg_loss(val_logits.values, val_labels.values, weight, row['Reg'])\n",
    "              \n",
    "        elif (row['Algo'] == 'logreg'):\n",
    "            src = models.LogisticReg()\n",
    "            coeffs = pd.read_pickle(row['regressors'])\n",
    "            weight = src.get_weight_norm(coeffs)\n",
    "            \n",
    "            train_logits = src.predict(train_data, coeffs)\n",
    "            train_loss = compute_linreg_loss(train_logits.values, train_labels.values, weight, row['Reg'])\n",
    "            val_logits = src.predict(val_data, coeffs)\n",
    "            val_loss = compute_linreg_loss(val_logits.values, val_labels.values, weight, row['Reg'])\n",
    "                                                                               \n",
    "        resdf.loc[resid, 'training_loss'] = train_loss    \n",
    "        resdf.loc[resid, 'validation_loss'] = val_loss\n",
    "    return resdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_hyperparameters(non_invariance_algos['logreg']['params'])\n",
    "res.drop('regressors', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = res[res['Dataset'] == 'adult']\n",
    "at = adult.sort_values(by=['training_loss'])\n",
    "av = adult.sort_values(by=['validation_loss'])\n",
    "\n",
    "german = res[res['Dataset'] == 'german']\n",
    "gt = german.sort_values(by=['training_loss'])\n",
    "gv = german.sort_values(by=['validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = av.drop(['Algo', 'Fteng', 'Dataset', 'ReduceDsize', 'Bin'], axis=1)\n",
    "tmp = av.groupby([\"TestSet\", 'Reg'])[['training_loss', 'validation_loss']].mean()\n",
    "tmp.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_hyperparameters(invariance_algos['linear_irm']['params'])\n",
    "res.drop('phi', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = res[res['Dataset'] == 'adult']\n",
    "at = adult.sort_values(by=['training_loss'])\n",
    "av = adult.sort_values(by=['validation_loss'])\n",
    "\n",
    "german = res[res['Dataset'] == 'german']\n",
    "gt = german.sort_values(by=['training_loss'])\n",
    "gv = german.sort_values(by=['validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = gv.drop(['Algo', 'Fteng', 'Dataset', 'ReduceDsize', 'Bin', 'Eq_Estrat', 'Envs'], axis=1)\n",
    "tmp = gv.groupby([\"TestSet\", 'LR', 'N_Iterations', 'PenWeight', 'HidLayers'])[['training_loss', 'validation_loss']].mean()\n",
    "tmp.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = res.groupby(['PenWeight', 'LR']).mean()\n",
    "# testing.head(10)\n",
    "t = res.sort_values(by=['training_loss'])\n",
    "v = res.sort_values(by=['validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig = ['Algo', 'Fteng', 'Dataset', \\\n",
    "#                                 'ReduceDsize', 'Bin', 'Eq_Estrat', \\\n",
    "#                                 'LR', 'N_Iterations', 'L2_WeightPen', \\\n",
    "#                                 'N_AnnealIter', 'PenWeight', 'HidLayers']\n",
    "# final = pd.DataFrame(columns=(orig + ['training_loss', 'validation_loss']))\n",
    "# i=0\n",
    "# for resid, row in v.iterrows():\n",
    "#     if (i < 500) and (row['Seed'] == 1000):  #and (row['Envs'] == ''):\n",
    "#         tmp = df_subset(v, row[orig])\n",
    "#         tmp = tmp.drop('Seed', axis=1)\n",
    "#         tmp = tmp.drop('Envs', axis=1)\n",
    "#         tmp = tmp.drop('TestSet', axis=1)\n",
    "#         tmp.loc[resid, 'training_loss'] = tmp['training_loss'].mean()\n",
    "#         tmp.loc[resid, 'validation_loss'] = tmp['validation_loss'].mean()\n",
    "#         final = final.append(tmp.loc[resid])\n",
    "#         i += 1\n",
    "# final = final.sort_values(by=['validation_loss'])\n",
    "# final = final.reset_index()\n",
    "# final.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_orig = ['Algo', 'Fteng', 'Dataset', \\\n",
    "#                                 'ReduceDsize', 'Bin', 'Eq_Estrat', \\\n",
    "#                                 'LR', 'N_Iterations', 'L2_WeightPen', \\\n",
    "#                                 'N_AnnealIter', 'PenWeight', 'HidLayers']\n",
    "# df_subset(final, final.loc[16][this_orig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Invariance Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invar_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Eq_Estrat', -1]] \n",
    "\n",
    "invar_COMPARED =  {'Envs':['workclass', 'native-country'], \\\n",
    "                   'Seed':[147, 256, 304],\n",
    "                   'Fteng':['1', '12'], \\\n",
    "                   'Bin':[1]}\n",
    "\n",
    "invar_orig_cols = [a[0] for a in invar_FIXED] + list(invar_COMPARED.keys()) + ['TestSet']\n",
    "invar_results = generate_all_existing_results(invar_orig_cols, \\\n",
    "                                             [invariance_algos[a]['params'] for a in list(invariance_algos.keys())])    \n",
    "#invar_results = generate_results(invar_FIXED, invar_COMPARED)\n",
    "\n",
    "invar_results = invar_results[invar_results['Seed'] == 1000]\n",
    "invar_results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TMPPPPPPPPPP\n",
    "\n",
    "a = invariance_algos['linear_irm']['params']\n",
    "invariance_algos['linear_irm']['params'] = a[(a['Seed'] == 1000) & (a['LR'] == 0.01) & (a['N_Iterations'] == 1000) \\\n",
    "                                            & (a['PenWeight'] == 1000) & (a['HidLayers'] == 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results(invariance_algos, invar_results, invar_orig_cols, from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 4000\n",
    "invar_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluating on Non-Invariance Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_FIXED = [['Dataset', 'adult'], \\\n",
    "               ['ReduceDsize', 10000], \\\n",
    "               ['Bin', 1]] \n",
    "\n",
    "var_COMPARED =  {'Fteng':['1', '12'], \\\n",
    "                 'Seed':[147, 256, 304]}\n",
    "\n",
    "var_orig_cols = [a[0] for a in var_FIXED] + list(var_COMPARED.keys()) + ['TestSet']\n",
    "var_results = generate_all_existing_results(var_orig_cols, \\\n",
    "                                             [non_invariance_algos[a]['params'] for a in list(non_invariance_algos.keys())]) \n",
    "# var_results = generate_results(var_FIXED, var_COMPARED)\n",
    "\n",
    "var_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compute_results(non_invariance_algos, var_results, var_orig_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 400\n",
    "var_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save To Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_resdir = 'latex_results'\n",
    "latex_fname = '0611_linearirm_german_hyp.xlsx'\n",
    "tmp.to_excel(os.path.join(latex_resdir, latex_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
